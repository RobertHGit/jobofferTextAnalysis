{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse job offer text\n",
    "*created on blog by Charlie Greenbacker [@greenbacker](https://twitter.com/greenbacker)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting text from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "---------------------------------------------\n",
      "---------------Job Description---------------\n",
      "---------------------------------------------\n",
      "Job descriptionOur Data Science team is divided into six key areas: Discovery, Supply Chain, Retail, Customer Care, Marketing and Core. We have a highly innovative team working on interesting and difficult problems, leveraging cutting edge technology to build solutions and customer facing products. Our team have a direct impact on shaping the future of how customers shop on ASOS, and help shape key business decisions and activities.And yeahâ€¦ we are pretty cutting edge with the technology we use here: GPU computing with Tensorflow and Keras. Distributed systems with pySpark and Databricks. Cloud computing with Azure: Docker, Kubernetes, data lake, CosmosDB.The team have recently also published papers, check them out here: Customer Lifetime Value Prediction with Embeddings, SIGKDD 17. Generalising Random Forest Parameter Optimisation to Include Stability and Cost, ECML 17. Product Characterisation towards Personalisation, SIGKDD 18. Predicting Twitter User Socioeconomic Attributes with Network and Language Information, ACM Hypertext and Social Media 18 We are looking for a Data Scientist to join our Discovery Science team. You will play a key role in helping ASOS to build out our discovery propositions, such as recommendation systems, personalisation algorithms and search engines. The role offers broad exposure to ASOS, requiring close collaboration with retail, marketing and technology divisions. You will be part of a highly innovative team working on interesting and difficult problems, leveraging cutting edge technology to build data products used by millions of customers. The ideal candidate will have a strong technical background and experience solving tough problems with large datasets. You will be a highly intelligent self-starter, a bias towards action and attention to detail.What You'll Be Doing ...Assist with the implementation and scale-up of algorithms for bespoke recommendation engines.Create prototypes for new personalisation features and turn machine-learning outputs into operational data products.Set up and conduct large-scale experiments to test hypotheses and drive product development.Leverage quantitative techniques to answer non-routine business problems.Analyse large, complex datasets to reveal underlying patterns, correlations and trends.We'd love to meet someone with...PhD or MSc degree or equivalent in Computer Science, Statistics, Engineering, Mathematics, Physics or similar quantitative discipline.Experience in solving large analytical problems using statistical, machine learning or other quantitative methods e.g. supervised unsupervised learning, graphical models, time series analysis, statistical testing A good understanding of statistics e.g., hypothesis testing, regressions Proficiency in one or more programming scripting languages e.g. Python, SQL Hive, Scala, R, C++ C#, Java, MATLAB, Perl, PHP Ability to work collaboratively and proactively in a fast paced environment interacting with both a non-technical and technical audienceYou may also have...Experience with distributed computing Hadoop Spark Previous experience with recommender systemsCommercial mindset and keen interest in E-commerce or the online fashion industryA hackers mentality and comfort using open source technologies and Unix scriptingInterest in keeping up with state-of-the-art machine learning by attending and submitting papers to relevant conferences.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "import os\n",
    "\n",
    "url = \"https://www.linkedin.com/jobs/view/892126521/\"\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.dirname('__file__'))\n",
    "DRIVER_BIN = os.path.join(PROJECT_ROOT, \"chromedriver\")\n",
    "\n",
    "driver = webdriver.Chrome(executable_path = DRIVER_BIN)\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "driver.implicitly_wait(20) # seconds\n",
    "element = driver.find_element_by_xpath('//*[@id=\"summary-detail\"]/div/div/div/div[1]/div')\n",
    "\n",
    "\n",
    "elementHTML = element.get_attribute('innerHTML')\n",
    "elementHTML = BeautifulSoup(elementHTML)\n",
    "\n",
    "text = elementHTML.get_text()\n",
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace('(', ' ')\n",
    "text = text.replace(')', ' ')\n",
    "text = text.replace('/', ' ')\n",
    "text = re.sub(' +',' ', text)\n",
    "\n",
    "#input('Press ENTER to close the automated browser')\n",
    "\n",
    "driver.quit()\n",
    "print('done')\n",
    "print('---------------------------------------------')\n",
    "print('---------------Job Description---------------')\n",
    "print('---------------------------------------------')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/roberthommes/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download nltk attribute if needed\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "filtered_text = [word for word in filtered_text if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = [''.join(c for c in s if c not in string.punctuation) for s in filtered_text]\n",
    "filtered_text = [s for s in filtered_text if s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = [word.lower() for word in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "filtered_text_stem = [stemmer.stem(t) for t in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "filtered_text_lem = [lemmatizer.lemmatize(t) for t in filtered_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------head words-----------------\n",
      "         word  count\n",
      "0        team      6\n",
      "1    customer      5\n",
      "2        data      5\n",
      "3  technology      5\n",
      "4         you      4\n",
      "5     product      4\n",
      "6     problem      4\n",
      "7        edge      3\n",
      "8     cutting      3\n",
      "9      highly      3\n",
      "-----------------tail words-----------------\n",
      "            word  count\n",
      "0    proposition      1\n",
      "1         search      1\n",
      "2         engine      1\n",
      "3          offer      1\n",
      "4          broad      1\n",
      "5       exposure      1\n",
      "6      requiring      1\n",
      "7          close      1\n",
      "8  collaboration      1\n",
      "9     conference      1\n"
     ]
    }
   ],
   "source": [
    "#nltkText = [filtered_text_stem, filtered_text_lem]\n",
    "\n",
    "fdist = nltk.FreqDist(filtered_text_lem)\n",
    "\n",
    "fdistDF = pd.DataFrame.from_dict(fdist, orient='index').reset_index()\n",
    "fdistDF = fdistDF.rename(columns={'index':'word', 0:'count'})\n",
    "fdistDF = fdistDF.sort_values(by='count', ascending=False)\n",
    "print('-----------------head words-----------------')\n",
    "print(fdistDF.head(10).reset_index(drop=True))\n",
    "print('-----------------tail words-----------------')\n",
    "print(fdistDF.tail(10).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze document words with data science job descripiton database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To indentify what skills are specific for this job vacancy in particulair we have to create a data science job description database. Using this database we can identify what is specific about this job with the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Term-Document Matrix\n",
    "\n",
    "Use scikit-learn's <code>TfidfVectorizer</code> class to construct a [term-document matrix](http://en.wikipedia.org/wiki/Document-term_matrix) containing the TF-IDF score for each word in each document in the data science job description database. In essence, the rows of this sparse matrix correspond to documents in the corpus, the columns represent each word in the vocabulary of the corpus, and each cell contains the TF-IDF value for a given word in a given document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Scores\n",
    "\n",
    "Now that we've built the term-document matrix, we can explore its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "print 'TDM contains ' + str(len(feature_names)) + ' terms and ' + str(tdm.shape[0]) + ' documents'\n",
    "\n",
    "print 'first term: ' + feature_names[0]\n",
    "print 'last term: ' + feature_names[len(feature_names) - 1]\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print 'random term: ' + feature_names[randint(1,len(feature_names) - 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the Summary\n",
    "\n",
    "That's all we'll need to produce a summary for any document in the corpus. In the example code below, we start by randomly selecting an article from the data science job description database. We iterate through the article, calculating a score for each sentence by summing the TF-IDF values for each word appearing in the sentence. We normalize the sentence scores by dividing by the number of tokens in the sentence (to avoid bias in favor of longer sentences). Then we sort the sentences by their scores, and return the highest-scoring sentences as our summary. The number of sentences returned corresponds to roughly 20% of the overall length of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from __future__ import division\n",
    "\n",
    "article_id = randint(0, tdm.shape[0] - 1)\n",
    "article_text = reuters.raw(reuters.fileids()[article_id])\n",
    "\n",
    "sent_scores = []\n",
    "for sentence in nltk.sent_tokenize(article_text):\n",
    "    score = 0\n",
    "    sent_tokens = tokenize_and_stem(sentence)\n",
    "    for token in (t for t in sent_tokens if t in feature_names):\n",
    "        score += tdm[article_id, feature_names.index(token)]\n",
    "    sent_scores.append((score / len(sent_tokens), sentence))\n",
    "\n",
    "summary_length = int(math.ceil(len(sent_scores) / 5))\n",
    "sent_scores.sort(key=lambda sent: sent[0], reverse=True)\n",
    "\n",
    "print '*** SUMMARY ***'\n",
    "for summary_sentence in sent_scores[:summary_length]:\n",
    "    print summary_sentence[1]\n",
    "\n",
    "print '\\n*** ORIGINAL ***'\n",
    "print article_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
